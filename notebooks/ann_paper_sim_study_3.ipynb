{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann-paper-sim-study-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfmWGoPce23hYy0fQowLYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DepartmentOfStatisticsPUE/ann-for-survey-sampling/blob/main/notebooks/ann_paper_sim_study_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsAD8OMvezI5"
      },
      "source": [
        "!apt install libomp-dev\n",
        "!pip install faiss-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Br6Vs_0fa34"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "from scipy.spatial import KDTree\n",
        "res = faiss.StandardGpuResources() "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF1eaae6rFUg"
      },
      "source": [
        "def kdtree_impute(tree, sample, data, y, x, eps = 0):\n",
        "  nns = tree.query(sample[:,x], k = 1, eps = eps)\n",
        "  res = np.mean(data[nns[1]][:, ys], axis = 0)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImvPed5q8km"
      },
      "source": [
        "Simulation study taken from: Kim, J. K., & Wang, Z. (2018). Sampling Techniques for Big Data Analysis. International Statistical Review, 1, 1–15. https://doi.org/10.1111/insr.12290"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik4LdbVDfMY_",
        "outputId": "f9287cd2-76ee-4fe5-b03b-59575190ed11"
      },
      "source": [
        "np.random.seed(123)\n",
        "N = 1000000\n",
        "x1 = np.random.normal(loc=1.0,scale=1.0,size=N)\n",
        "x2 = np.random.exponential(scale=1.0, size = N)\n",
        "epsilon = np.random.normal(size=N)\n",
        "\n",
        "### target variables\n",
        "y1 = 1 + x1 + x2 + epsilon\n",
        "y2 = 0.5*(x1 - 1.5)**2 + x2 + epsilon\n",
        "## propensity scores\n",
        "p1 = np.exp(x2) / (1 + np.exp(x2))\n",
        "p2 = np.exp(-0.5 + 0.5*(x2-2)**2) / (1 + np.exp(-0.5 + 0.5*(x2-2)**2))\n",
        "\n",
        "data = np.column_stack((x1,x2,y1,y2,p1,p2)).astype('float32')\n",
        "data[:3]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0856306 ,  0.5546646 ,  0.90054107,  1.2432839 ,  0.63521713,\n",
              "         0.632858  ],\n",
              "       [ 1.9973454 ,  1.1975824 ,  4.455222  ,  1.5815529 ,  0.7680944 ,\n",
              "         0.4556015 ],\n",
              "       [ 1.2829785 ,  0.84342945,  3.0256119 ,  0.76618266,  0.699187  ,\n",
              "         0.542107  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xtafAIGfqcI"
      },
      "source": [
        "R = 500\n",
        "xs = [0,1]\n",
        "ys = [2,3]\n",
        "ps = [4,5]\n",
        "\n",
        "results_faiss_500 = np.zeros(shape = (R, 2))\n",
        "results_faiss_1000 = np.zeros(shape = (R, 2))\n",
        "results_kdtree_500 = np.zeros(shape = (R, 2))\n",
        "results_kdtree_1000 = np.zeros(shape = (R, 2))\n",
        "\n",
        "for r in range(R):\n",
        "  print(r)\n",
        "  np.random.seed(r)\n",
        "  ## big data sample\n",
        "  big_p1 = np.random.binomial(n=1, p = p1, size = N)\n",
        "  big_p2 = np.random.binomial(n=1, p = p2, size = N)    \n",
        "  ## random samples\n",
        "  s500 = np.random.choice(a = data.shape[0], size = 500, replace = False)\n",
        "  s1000 = np.random.choice(a = data.shape[0], size = 1000, replace = False)\n",
        "  ## kdtree (exact)\n",
        "  kdtree = KDTree(data[big_p1==1][:, xs], leafsize = 10)\n",
        "  results_kdtree_500[r, :] = kdtree_impute(kdtree, data[s500], data[big_p1==1], ys, xs)\n",
        "  results_kdtree_1000[r, :] = kdtree_impute(kdtree, data[s1000], data[big_p1==1], ys, xs)\n",
        "  ## faiss\n",
        "  big_data = data[big_p1==1][:, xs].copy()\n",
        "  sam_data_500 = data[s500][:, xs].copy()\n",
        "  sam_data_1000 = data[s1000][:, xs].copy()\n",
        "  index_flat = faiss.IndexFlatL2(len(xs))\n",
        "  gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
        "  gpu_index_flat.add(big_data)\n",
        "  D_500, I_500 = gpu_index_flat.search(sam_data_500, k = 1) \n",
        "  D_1000, I_1000 = gpu_index_flat.search(sam_data_1000, k = 1) \n",
        "  ind_500 = [i[0] for i in I_500]\n",
        "  ind_1000 = [i[0] for i in I_1000]\n",
        "  results_faiss_500[r,:]=np.mean(data[big_p1==1][ind_500][:, ys], axis=0)\n",
        "  results_faiss_1000[r,:]=np.mean(data[big_p1==1][ind_1000][:, ys], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhEJCYsWlZNW",
        "outputId": "f24d592a-1f45-47a7-b38e-6283f8c097fe"
      },
      "source": [
        "expected = np.stack(\n",
        "    [np.mean(results_kdtree_500, axis=0),\n",
        "     np.mean(results_faiss_500, axis=0),\n",
        "     np.mean(results_kdtree_1000, axis=0),\n",
        "     np.mean(results_faiss_1000, axis=0)\n",
        "     ]\n",
        ") \n",
        "\n",
        "stderrs =  np.stack(\n",
        "    [np.std(results_kdtree_500, axis=0),\n",
        "     np.std(results_faiss_500, axis=0),\n",
        "     np.std(results_kdtree_1000, axis=0),\n",
        "     np.std(results_faiss_1000, axis=0)\n",
        "     ]\n",
        ")\n",
        "\n",
        "bias = expected - np.mean(data[:,ys], axis=0)\n",
        "mse = bias**2 + stderrs**2\n",
        "\n",
        "print(\"===== bias =====\")\n",
        "print(bias)\n",
        "print(\"===== se =====\")\n",
        "print(stderrs)\n",
        "print(\"===== mse =====\")\n",
        "print(mse)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== bias =====\n",
            "[[ 0.00424795 -0.00285448]\n",
            " [ 0.00378274 -0.00331977]\n",
            " [ 0.00329126  0.00082333]\n",
            " [ 0.00325391  0.00078617]]\n",
            "===== se =====\n",
            "[[0.07272113 0.07345612]\n",
            " [0.07295542 0.07315786]\n",
            " [0.05202406 0.04801981]\n",
            " [0.05191877 0.04749557]]\n",
            "===== mse =====\n",
            "[[0.00530641 0.00540395]\n",
            " [0.0053368  0.00536309]\n",
            " [0.00271734 0.00230658]\n",
            " [0.00270615 0.00225645]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czxaj3oawu6m"
      },
      "source": [
        "## save results\n",
        "results = {\n",
        "    \"results_kdtree_500\" : pd.DataFrame(results_kdtree_500),\n",
        "    \"results_faiss_500\" : pd.DataFrame(results_faiss_500),\n",
        "    \"results_kdtree_1000\": pd.DataFrame(results_kdtree_1000),\n",
        "    \"results_faiss_1000\": pd.DataFrame(results_faiss_1000),\n",
        "           }\n",
        "\n",
        "f = open(\"kdtree_faiss_500_1000k.pkl\",\"wb\")\n",
        "pickle.dump(results,f)\n",
        "f.close()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGA2ywP5reji"
      },
      "source": [
        "Modified case from: Yang, S., & Kim, J. K. (2019). Nearest neighbour imputation for general parameter estimation in survey sampling. In The Econometrics of Complex Survey Data: Theory and Applications (Vol. 39, pp. 209–234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvq0MP_BsuZI"
      },
      "source": [
        "Study from Yang, S., & Kim, J. K. (2020). Doubly robust inference when combining probability and non-probability samples with high dimensional. Journal of the Royal Statistical Society. Series B: Statistical Methodology, 82(2), 445–465. https://doi.org/10.1111/rssb.12354\n",
        "\n",
        "R code (source: https://github.com/shuyang1987/IntegrativeFPM)\n",
        "\n",
        "```r\n",
        "set.seed(1234)\n",
        "## population size\n",
        "N <- 10000\n",
        "## x is a p-dimensional covariate\n",
        "p <- 50\n",
        "x <- matrix( rnorm(N*p,0,1),N,p)\n",
        "## y is a continuous outcome \n",
        "beta0 <- c(1,1,1,1,1,rep(0,p-4))\n",
        "y <- cbind(1,x)%*%beta0 + rnorm(N,0,1)\n",
        "true <- mean(y)\n",
        "## y2 is a binary outcome\n",
        "ly2 <- (cbind(1,x)%*%beta0)\n",
        "ply <- exp(ly2)/(1+exp(ly2))\n",
        "y2 <- rbinom(N,1,ply)\n",
        "true2 <- mean(y2)\n",
        "## A.set is a prob sample: SRS\n",
        "## sampling probability into A is known when estimation\n",
        "nAexp <- 1000\n",
        "probA <- rep(nAexp/N,N)\n",
        "A.index <- rbinom(N,size = 1,prob = probA)\n",
        "A.loc <- which(A.index == 1)\n",
        "nA <- sum(A.index == 1)\n",
        "sw.A <- 1/probA[A.loc]\n",
        "x.A <- x[A.loc,]\n",
        "y.A <- rep(NA,nA) # y is not observed in Sample A\n",
        "y2.A <- rep(NA,nA)\n",
        "## B.set is a nonprob sample\n",
        "## sampling probability into B is unknown when estimation\n",
        "nBexp <- 2000\n",
        "alpha0 <- c(-2,1,1,1,1,rep(0,p-4))\n",
        "probB <- (1+exp(-cbind(1,x)%*%alpha0))^(-1) \n",
        "B.index <- rbinom(N,size = 1,prob = probB)\n",
        "B.loc <- which(B.index == 1)\n",
        "nB <- sum(B.index)\n",
        "x.B <- x[B.loc,]\n",
        "y.B <- y[B.loc]\n",
        "y2.B <- y2[B.loc]\n",
        "## combined dataset\n",
        "y.AB <- c(y.A,y.B)\n",
        "y2.AB <- c(y2.A,y2.B)\n",
        "x.AB <- rbind(x.A,x.B)\n",
        "deltaB <- c(rep(0,nA),rep(1,nB))\n",
        "sw <- c(sw.A,rep(1,nB))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERf_wTZ3reNq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}