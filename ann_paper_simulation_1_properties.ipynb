{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann-paper-simulation-1-properties.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BaOylZ_wyYzY"
      ],
      "authorship_tag": "ABX9TyM+vrXdziGe1NwJ6h1hFjXV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DepartmentOfStatisticsPUE/ann-for-survey-sampling/blob/main/ann_paper_simulation_1_properties.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaOylZ_wyYzY"
      },
      "source": [
        "## About\n",
        "\n",
        "This notebook covers simulation study to assess the performance of predictive mean imputation based on exact and approximate nearest neigbours. \n",
        "\n",
        "**Warning**: before runing this scripts go to `Runtime` -> `Change runtime type` and set it to `GPU`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAJ8fCEMyxzQ"
      },
      "source": [
        "## Install requested modules\n",
        "\n",
        "Please note that this may take some time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIGVVBPyyPUw"
      },
      "source": [
        "!apt install libomp-dev\n",
        "!pip install faiss-gpu\n",
        "!pip install n2\n",
        "!pip install scann\n",
        "!pip install annoy ## takes minutes to add data and index\n",
        "!pip install pyflann-py3\n",
        "## !pip install pynndescent -- not suitable for PMM 1d dimension: gets error \"no suitable hyperplains were found\"\n",
        "\n",
        "## this line cleanes information about installing\n",
        "## comment these lines if you want to see the progress \n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pMhr7DWznfJ"
      },
      "source": [
        "## Import requested modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYX-DeZRzqzU"
      },
      "source": [
        "## standard modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "## linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "## ann modules\n",
        "import scann\n",
        "import faiss\n",
        "from pyflann import *\n",
        "from annoy import AnnoyIndex\n",
        "from n2 import HnswIndex\n",
        "from scipy.spatial import cKDTree\n",
        "from pynndescent import NNDescent\n",
        "\n",
        "## serialization\n",
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkdqmjIT195F"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHUQPEzS1_a3"
      },
      "source": [
        "def kdtree_impute(y_pred, y_pred_miss, y):\n",
        "  tree = cKDTree(y_pred, leafsize = 100, balanced_tree=True)\n",
        "  dists, indx = tree.query(y_pred_miss, k = 1, eps = 0)\n",
        "  res = (np.sum(y) + np.sum(y[indx])) / (len(y_pred) + len(y_pred_miss))\n",
        "  return res\n",
        "\n",
        "def faiss_impute(y_pred, y_pred_miss, y, gpu = True, voronoi = False):\n",
        "  index_flat = faiss.IndexFlatL2(1)\n",
        "\n",
        "  if voronoi:\n",
        "    index_flat = faiss.IndexIVFFlat(index_flat, 1, 1000)\n",
        "\n",
        "  if gpu:\n",
        "    gpu_faiss = faiss.StandardGpuResources() \n",
        "    index_flat = faiss.index_cpu_to_gpu(gpu_faiss, 0, index_flat)\n",
        "  \n",
        "  if voronoi:\n",
        "    index_flat.train(y_pred)\n",
        "\n",
        "  index_flat.add(y_pred)\n",
        "  dists, indx = index_flat.search(y_pred_miss, k = 1) \n",
        "  res = (np.sum(y) + np.sum(y[indx])) / (len(y_pred) + len(y_pred_miss))\n",
        "  return res\n",
        "\n",
        "def annoy_impute(y_pred, y_pred_miss, y, trees = 50):\n",
        "  t = AnnoyIndex(1, \"euclidean\") \n",
        "  for i in range(len(y_pred)):\n",
        "    t.add_item(i, y_pred[i]) \n",
        "\n",
        "  t.build(trees)\n",
        "  indx = np.array([t.get_nns_by_vector(i, 1) for i in y_pred_miss])\n",
        "  res = (np.sum(y) + np.sum(y[indx])) / (len(y_pred) + len(y_pred_miss))\n",
        "  return res\n",
        "\n",
        "def n2_impute(y_pred, y_pred_miss, y, trees = 50):\n",
        "  t = HnswIndex(1, \"euclidean\") \n",
        "  for i in y_pred:\n",
        "    t.add_data(i)\n",
        "\n",
        "  t.build(m=5, n_threads=-1)\n",
        "  indx = t.batch_search_by_vectors(y_pred_miss, 1)\n",
        "  res = (np.sum(y) + np.sum(y[indx])) / (len(y_pred) + len(y_pred_miss))\n",
        "  return res\n",
        "\n",
        "def flann_impute(y_pred, y_pred_miss, y):\n",
        "  flann = FLANN()\n",
        "  indx, dists = flann.nn(y_pred, y_pred_miss, 1, \n",
        "                       algorithm=\"kmeans\", branching=32, iterations=7, checks=16)\n",
        "  res = (np.sum(y) + np.sum(y[indx])) / (len(y_pred) + len(y_pred_miss))\n",
        "  return res\n",
        "\n",
        "## to be corrected\n",
        "def scann_impute(y_pred, y_pred_miss, y):\n",
        "  searcher = scann.scann_ops_pybind.builder(y_pred, 1, \"squared_l2\").tree(\n",
        "      num_leaves=1000, num_leaves_to_search=50, training_sample_size=5000).score_ah(\n",
        "          2, anisotropic_quantization_threshold=0.2).reorder(10).build()\n",
        "  indx, nns = searcher.search_batched(y_pred_miss)\n",
        "  res = (np.sum(y) + np.sum(y[indx])) / (len(y_pred) + len(y_pred_miss))\n",
        "  return res\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC1nDkky0xZy"
      },
      "source": [
        "## Simulation studies outline\n",
        "\n",
        "Here, we conduct simulation study based on predictive mean matching. We replicate study from *Yang, S., & Kim, J. K. (2020). Asymptotic theory and inference of predictive mean matching imputation using a superpopulation model framework. Scandinavian Journal of Statistics, 47(3), 839-861.* paper, however we only assume missing data mechanism\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZdPhEJG13m4",
        "outputId": "9dd6bc85-57d2-41ed-dbb2-d24d4c87ab9e"
      },
      "source": [
        "np.random.seed(123)\n",
        "N = 50000\n",
        "x1 = np.random.uniform(size = N)\n",
        "x2 = np.random.uniform(size = N)\n",
        "x3 = np.random.uniform(size = N)\n",
        "x4 = np.random.normal(size = N)\n",
        "x5 = np.random.normal(size = N)\n",
        "x6 = np.random.normal(size = N)\n",
        "epsilon = np.random.normal(size=N)\n",
        "\n",
        "### target variables\n",
        "y1 = -1 + x1 + x2 + epsilon\n",
        "y2 = -1.167 + x1 + x2 + (x1 - 0.5)**2 + + (x2 - 0.5)**2 + epsilon\n",
        "y3 = -1.5 + x1 + x2 + x3 + x4 + x5 + x6 + epsilon\n",
        "\n",
        "## response indicator\n",
        "p1 = np.exp(0.2 + x1 + x2) / (1 + np.exp(0.2 + x1 + x2))\n",
        "\n",
        "data = np.column_stack((x1,x2,x3,x4,x5,x6,y1,y2,y3, p1)).astype('float32')\n",
        "\n",
        "## first three rows\n",
        "data[:3]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.6964692 ,  0.36086547,  0.20932843,  0.47665665,  0.80429375,\n",
              "        -0.9894137 ,  0.42543107,  0.31638962,  0.42629617,  0.77856696],\n",
              "       [ 0.28613934,  0.22535679,  0.2937351 ,  0.6701647 ,  1.2931948 ,\n",
              "         1.033963  ,  0.25331086,  0.20747614,  3.0443685 ,  0.67073166],\n",
              "       [ 0.22685145,  0.50813043,  0.05571789,  0.5033514 ,  1.5591047 ,\n",
              "         0.13504769,  0.36729714,  0.27497336,  2.1205187 ,  0.71808493]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1LemmH54jxh"
      },
      "source": [
        "## Simulation study\n",
        "\n",
        "Note, that this simulation may take several hours and you need to be connected to the Interent.\n",
        "\n",
        "Results are stored in a `pickle` format that is naive for `python`'s `DataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_4IDfhK23Ea",
        "outputId": "0a2d27b4-985b-45bb-a9f6-c7190358e83e"
      },
      "source": [
        "R = 500\n",
        "sim1_results_ckdtree = np.zeros(shape = (R, 3))\n",
        "sim1_results_faiss = np.zeros(shape = (R, 3))\n",
        "sim1_results_faiss_v = np.zeros(shape = (R, 3))\n",
        "sim1_results_annoy = np.zeros(shape = (R, 3))\n",
        "sim1_results_n2 = np.zeros(shape = (R, 3))\n",
        "sim1_results_flann = np.zeros(shape = (R, 3))\n",
        "sim1_results_scann = np.zeros(shape = (R, 3))\n",
        "#sim1_results_ckdtree_time\n",
        "\n",
        "for r in range(R):\n",
        "  \n",
        "  if (r % 10 == 0):\n",
        "    print(r)\n",
        "\n",
        "  np.random.seed(r)\n",
        "  response_flag = np.random.binomial(n=1, p = p1, size = N)\n",
        "  data_resp = data[response_flag == 1]\n",
        "  data_noresp = data[response_flag != 1]\n",
        "  \n",
        "  ## predictive mean matching\n",
        "  ## y1\n",
        "  m1_reg_y1 = LinearRegression().fit(data_resp[:,:2], data_resp[:, 6])\n",
        "  m1_resp_y1_predict = m1_reg_y1.predict(data_resp[:,:2]).reshape(-1,1)\n",
        "  m1_noresp_y1_predict = m1_reg_y1.predict(data_noresp[:,:2]).reshape(-1,1)\n",
        "  ## y2\n",
        "  m1_reg_y2 = LinearRegression().fit(data_resp[:,:2], data_resp[:, 7])\n",
        "  m1_resp_y2_predict = m1_reg_y2.predict(data_resp[:,:2]).reshape(-1,1)\n",
        "  m1_noresp_y2_predict = m1_reg_y2.predict(data_noresp[:,:2]).reshape(-1,1)\n",
        "  ## y3\n",
        "  m1_reg_y3 = LinearRegression().fit(data_resp[:,:6], data_resp[:, 8])\n",
        "  m1_resp_y3_predict = m1_reg_y3.predict(data_resp[:,:6]).reshape(-1,1)\n",
        "  m1_noresp_y3_predict = m1_reg_y3.predict(data_noresp[:,:6]).reshape(-1,1)\n",
        "\n",
        "  ## cktree imputation\n",
        "  sim1_results_ckdtree[r, 0] = kdtree_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "  sim1_results_ckdtree[r, 1] = kdtree_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7])\n",
        "  sim1_results_ckdtree[r, 2] = kdtree_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8])\n",
        "  \n",
        "  ## faiss imputation\n",
        "  sim1_results_faiss[r, 0] = faiss_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "  sim1_results_faiss[r, 1] = faiss_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7])\n",
        "  sim1_results_faiss[r, 2] = faiss_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8])\n",
        "\n",
        "  ## faiss imputation using voronoi\n",
        "  sim1_results_faiss_v[r, 0] = faiss_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6],voronoi = True) \n",
        "  sim1_results_faiss_v[r, 1] = faiss_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7],voronoi = True)\n",
        "  sim1_results_faiss_v[r, 2] = faiss_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8],voronoi = True)\n",
        "\n",
        "  ## annoy imputation\n",
        "  sim1_results_annoy[r, 0] = annoy_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "  sim1_results_annoy[r, 1] = annoy_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7])\n",
        "  sim1_results_annoy[r, 2] = annoy_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8])\n",
        "\n",
        "  ## n2 imputation\n",
        "  sim1_results_n2[r, 0] = n2_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "  sim1_results_n2[r, 1] = n2_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7])\n",
        "  sim1_results_n2[r, 2] = n2_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8])\n",
        "\n",
        "  ## flann imputation\n",
        "  sim1_results_flann[r, 0] = flann_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "  sim1_results_flann[r, 1] = flann_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7])\n",
        "  sim1_results_flann[r, 2] = flann_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8])\n",
        "\n",
        "  ## scann imputation\n",
        "  sim1_results_scann[r, 0] = scann_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "  sim1_results_scann[r, 1] = scann_impute(m1_resp_y2_predict, m1_noresp_y2_predict, data_resp[:, 7])\n",
        "  sim1_results_scann[r, 2] = scann_impute(m1_resp_y3_predict, m1_noresp_y3_predict, data_resp[:, 8])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n",
            "470\n",
            "480\n",
            "490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gis7O3A9IXry"
      },
      "source": [
        "[abs(np.mean(sim1_results_ckdtree, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0),\n",
        " abs(np.mean(sim1_results_faiss, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0),\n",
        " abs(np.mean(sim1_results_faiss_v, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0),\n",
        " abs(np.mean(sim1_results_annoy, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0),\n",
        " abs(np.mean(sim1_results_n2, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0),\n",
        " abs(np.mean(sim1_results_flann, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0),\n",
        " abs(np.mean(sim1_results_scann, axis = 0) - np.mean(data[:,[6,7,8]], axis = 0)) / np.mean(data[:,[6,7,8]], axis = 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuylG-z4UvF4"
      },
      "source": [
        "Save results to `dict` object and write to `pkl` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-WfL6v4FWDH"
      },
      "source": [
        "results = {\n",
        "    \"data\": pd.DataFrame(data),\n",
        "    \"sim1_results_ckdtree\" : pd.DataFrame(sim1_results_ckdtree),\n",
        "    \"sim1_results_faiss_v\" : pd.DataFrame(sim1_results_faiss),\n",
        "    \"sim1_results_faiss_v\" : pd.DataFrame(sim1_results_faiss_v),\n",
        "    \"sim1_results_annoy\": pd.DataFrame(sim1_results_annoy),\n",
        "    \"sim1_results_n2\": pd.DataFrame(sim1_results_n2),\n",
        "    \"sim1_results_flann\" : pd.DataFrame(sim1_results_flann),\n",
        "    \"sim1_results_scann\": pd.DataFrame(sim1_results_scann)\n",
        "          }\n",
        "\n",
        "f = open(\"sim1-results.pkl\",\"wb\")\n",
        "pickle.dump(results,f)\n",
        "f.close()\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaYiKC-fkws6"
      },
      "source": [
        "## Processing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC1Ao7nTky9B",
        "outputId": "5ff37cd6-3ede-4d27-e2f6-9e65f62a2a7c"
      },
      "source": [
        "results = pd.read_pickle(\"sim1-results.pkl\")\n",
        "results.keys()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'sim1_results_ckdtree', 'sim1_results_faiss', 'sim1_results_annoy', 'sim1_results_n2', 'sim1_results_flann', 'sim1_results_faiss_v', 'sim1_results_scann'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgImjQnVk7s1"
      },
      "source": [
        "Calculate bias and standard error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6fxxTx1lBNo",
        "outputId": "0c8cd1bf-fa34-4d0e-cd4f-47cbb48d0a08"
      },
      "source": [
        "ys_true = np.array(np.mean(results[\"data\"].loc[:, [6, 7, 8]], axis = 0))\n",
        "ys_true"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00049869, -0.00018652,  0.01020216], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY6-Rjo5lcPY",
        "outputId": "3c003a28-270b-4ac9-9f37-a586185d33ec"
      },
      "source": [
        "algos = [\"sim1_results_ckdtree\", \"sim1_results_faiss\", \"sim1_results_faiss_v\",\n",
        "         \"sim1_results_annoy\", \"sim1_results_n2\", \"sim1_results_flann\", \n",
        "         \"sim1_results_scann\"]\n",
        "bias = np.zeros(shape = (len(algos), 3))\n",
        "vars = np.zeros(shape = (len(algos), 3))\n",
        "\n",
        "for i, a in enumerate(algos):\n",
        "  bias[i,:] = np.array(np.mean(results[a], axis = 0)) - ys_true\n",
        "  vars[i,:] = np.array(np.var(results[a], axis = 0))\n",
        "\n",
        "ses = np.sqrt(vars)\n",
        "\n",
        "algos_names = [str(i).replace(\"sim1_results_\", \"\") for i in algos]\n",
        "\n",
        "tab_report = np.column_stack((bias[:, 0], ses[:, 0], bias[:, 1], ses[:, 1], bias[:, 2], ses[:, 2]))\n",
        "\n",
        "print(pd.DataFrame(np.round(tab_report*1000,4), \n",
        "                    index = algos_names, \n",
        "                    columns = [\"bias_y1\", \"se_y1\", \"bias_y2\", \"se_y2\", \"bias_y3\", \"se_y3\"]).to_latex())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\begin{tabular}{lrrrrrr}\n",
            "\\toprule\n",
            "{} &  bias\\_y1 &    se\\_y1 &  bias\\_y2 &    se\\_y2 &  bias\\_y3 &    se\\_y3 \\\\\n",
            "\\midrule\n",
            "ckdtree &  -0.1764 &   3.4041 &  -0.1932 &   3.9165 &   0.0090 &   3.6757 \\\\\n",
            "faiss   &  -0.5274 &   3.4219 &  -0.5769 &   3.9145 &  -0.0839 &   3.6274 \\\\\n",
            "faiss\\_v &  -0.1765 &   3.4657 &  -0.2439 &   3.9018 &   0.0638 &   3.6788 \\\\\n",
            "annoy   &  -0.1747 &   3.4108 &  -0.1824 &   3.9220 &   0.0078 &   3.6805 \\\\\n",
            "n2      &  -0.1730 &   3.4020 &  -0.1867 &   3.9200 &   0.0101 &   3.6781 \\\\\n",
            "flann   &  -0.1771 &   3.4258 &  -0.1876 &   3.9400 &   0.0280 &   3.6748 \\\\\n",
            "scann   &  -4.4243 &  14.2124 &  -3.3353 &  14.5755 &  -5.6344 &  11.4536 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEUEyKq7WT8W"
      },
      "source": [
        "# Timings and costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQvs10e4Y9gw"
      },
      "source": [
        "Timings for N = 50 000 (previously generated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6mxxQSGWYMr",
        "outputId": "251a48ce-78d8-4006-dcb8-965a5af8247c"
      },
      "source": [
        "%timeit -n 10 kdtree_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6])  \n",
        "%timeit -n 10 faiss_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "%timeit -n 10 faiss_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6], voronoi = True) \n",
        "%timeit -n 10 faiss_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6], gpu = False)\n",
        "%timeit -n 10 annoy_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "%timeit -n 10 n2_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "%timeit -n 10 flann_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "%timeit -n 10 scann_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 43.5 ms per loop\n",
            "10 loops, best of 3: 169 ms per loop\n",
            "10 loops, best of 3: 210 ms per loop\n",
            "10 loops, best of 3: 1.24 s per loop\n",
            "10 loops, best of 3: 4.86 s per loop\n",
            "10 loops, best of 3: 2.74 s per loop\n",
            "10 loops, best of 3: 1.08 s per loop\n",
            "10 loops, best of 3: 664 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf89J1knZ-0O"
      },
      "source": [
        "Timings for N = 1 000 000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypWXBMJhZ-Oq"
      },
      "source": [
        "np.random.seed(123)\n",
        "N = 1000000\n",
        "x1 = np.random.uniform(size = N)\n",
        "x2 = np.random.uniform(size = N)\n",
        "x3 = np.random.uniform(size = N)\n",
        "x4 = np.random.normal(size = N)\n",
        "x5 = np.random.normal(size = N)\n",
        "x6 = np.random.normal(size = N)\n",
        "epsilon = np.random.normal(size=N)\n",
        "y1 = -1 + x1 + x2 + epsilon\n",
        "y2 = -1.167 + x1 + x2 + (x1 - 0.5)**2 + + (x2 - 0.5)**2 + epsilon\n",
        "y3 = -1.5 + x1 + x2 + x3 + x4 + x5 + x6 + epsilon\n",
        "p1 = np.exp(0.2 + x1 + x2) / (1 + np.exp(0.2 + x1 + x2))\n",
        "data = np.column_stack((x1,x2,x3,x4,x5,x6,y1,y2,y3, p1)).astype('float32')\n",
        "response_flag = np.random.binomial(n=1, p = p1, size = N)\n",
        "data_resp = data[response_flag == 1]\n",
        "data_noresp = data[response_flag != 1]\n",
        "m1_reg_y1 = LinearRegression().fit(data_resp[:,:2], data_resp[:, 6])\n",
        "m1_resp_y1_predict = m1_reg_y1.predict(data_resp[:,:2]).reshape(-1,1)\n",
        "m1_noresp_y1_predict = m1_reg_y1.predict(data_noresp[:,:2]).reshape(-1,1)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o_Eq4LmaNp4",
        "outputId": "78664b6e-49ba-4880-8017-251f307f2bd0"
      },
      "source": [
        "%timeit -n 10 kdtree_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6])  \n",
        "%timeit -n 10 faiss_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) \n",
        "%timeit -n 10 scann_impute(m1_resp_y1_predict, m1_noresp_y1_predict, data_resp[:, 6]) "
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 3.38 s per loop\n",
            "10 loops, best of 3: 10.1 s per loop\n",
            "10 loops, best of 3: 13.9 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}